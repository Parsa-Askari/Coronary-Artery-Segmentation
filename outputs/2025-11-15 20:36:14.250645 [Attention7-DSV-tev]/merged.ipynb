{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import albumentations as A\n",
    "from skimage.morphology import skeletonize\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "import zarr\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a517f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# In[1]:\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "#!/usr/bin/env python\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df95ec",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783de5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetDataset(Dataset):\n",
    "    def __init__(self,transform,data,base_size=512,out_counts=7):\n",
    "        super(UnetDataset,self).__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.to_tensor = ToTensorV2()\n",
    "        self.resizers = [\n",
    "            A.Resize(base_size//(2**i),base_size//(2**i),\n",
    "            interpolation=cv2.INTER_NEAREST,\n",
    "            mask_interpolation=cv2.INTER_NEAREST) for i in range(1,out_counts)\n",
    "        ]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "        img , mask = self.data[index]\n",
    "        img = np.expand_dims(img, axis=-1) \n",
    "        mask = mask[...,None]\n",
    "        result = self.transform(image=img, mask=mask)\n",
    "        new_image = result['image']\n",
    "        new_mask = result['mask'].squeeze(-1)\n",
    "\n",
    "        new_masks =[new_mask] + [resizer(image = new_mask)[\"image\"] for resizer in self.resizers]\n",
    "\n",
    "\n",
    "        new_image = self.to_tensor(image = new_image)[\"image\"]\n",
    "        new_masks = [torch.tensor(m).long() for m in new_masks]\n",
    "        return new_image.float() , new_masks\n",
    "\n",
    "class ValidUnetDataset(Dataset):\n",
    "    def __init__(self,transform,data):\n",
    "        super(ValidUnetDataset,self).__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "        img , mask = self.data[index]\n",
    "        img = np.expand_dims(img, axis=-1) \n",
    "        mask = mask[...,None]\n",
    "        result = self.transform(image=img, mask=mask)\n",
    "        new_image = result['image']\n",
    "        new_mask = result['mask'].squeeze(-1)\n",
    "\n",
    "        return new_image.float() , [new_mask.long()]\n",
    "\n",
    "class UnetExampleDataset(Dataset):\n",
    "    def __init__(self,transform,data,base_transform=None):\n",
    "        super(UnetExampleDataset,self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        if(base_transform is None):\n",
    "            self.base_transform = A.Compose([ToTensorV2()])\n",
    "        else:\n",
    "            self.base_transform = base_transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "        img , mask = self.data[index]\n",
    "        img = np.expand_dims(img, axis=-1) \n",
    "        result = self.transform(image=img, mask=mask)\n",
    "        new_image = result['image']\n",
    "        new_mask = result['mask']\n",
    "        \n",
    "        raw_result = self.base_transform(image=img, mask=mask)\n",
    "        raw_image = raw_result['image']\n",
    "        raw_mask = raw_result['mask']\n",
    "        return new_image.float() , new_mask , raw_image.float() , raw_mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_transforms = A.Compose([\n",
    "        A.GaussianBlur(\n",
    "            sigma_limit=[0.1,0.5],\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.1,\n",
    "            contrast_limit=0.15,\n",
    "            brightness_by_max=True,\n",
    "            p=0.3\n",
    "        ),\n",
    "        A.RandomGamma(\n",
    "            gamma_limit=(90, 120), \n",
    "            p=0.3\n",
    "        ),\n",
    "        A.Rotate(limit=15, p=0.3 , fill_mask = 0),\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        # A.Lambda(image=normalize_xca),\n",
    "        ]\n",
    "    )\n",
    "    ds = UnetDataset(transform=train_transforms,data = [[np.random.rand(512,512),np.random.rand(512,512)],[np.random.rand(512,512),np.random.rand(512,512)]])\n",
    "    dl = DataLoader(ds,batch_size=2)\n",
    "    for img , masks in dl:\n",
    "        print(img.shape)\n",
    "        for mask in masks:\n",
    "            print(mask.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f906bfa",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(base_path, part,preprocessor,max_workers=None):\n",
    "    base_path = Path(base_path)\n",
    "    images_base = base_path / \"images\" / part\n",
    "    labels_base = base_path / \"labels\" / part\n",
    "    skels_base = base_path / \"skels\" / part\n",
    "\n",
    "    image_names = sorted([p.name for p in os.scandir(images_base) if p.is_file()])\n",
    "    if(not preprocessor):\n",
    "        print(\"NOTE : preprocessor is not defined . no preprocessing will be used !\")\n",
    "    def _read_one(fname):\n",
    "        name_stem = Path(fname).stem\n",
    "        img_path = images_base / fname\n",
    "        label_path = labels_base / f\"{name_stem}.zarr\"\n",
    "        # skel_path = skels_base / fname\n",
    "\n",
    "        # skel_img = cv2.imread(str(skel_path), cv2.IMREAD_GRAYSCALE)/255.0\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if(preprocessor):\n",
    "            img = preprocessor(img)\n",
    "        label = zarr.load(str(label_path))\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    if max_workers is None:\n",
    "        cpu = os.cpu_count() or 4\n",
    "        max_workers = min(32, cpu * 4)\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for img, label in tqdm(ex.map(_read_one, image_names), total=len(image_names)):\n",
    "            results.append([img,label])\n",
    "\n",
    "    return results\n",
    "\n",
    "def to_device(img,gt_mask,device,binary_mode):\n",
    "    gt_mask = gt_mask.long()\n",
    "    img = img.to(device)\n",
    "    gt_mask = gt_mask.to(device)\n",
    "    if(binary_mode):\n",
    "        gt_label = gt_label.to(device)\n",
    "    else :\n",
    "        gt_label = None\n",
    "    return img , gt_mask \n",
    "\n",
    "\n",
    "def crop_dims(target , current):\n",
    "    left = (current.shape[3]-target.shape[3])//2\n",
    "    right = (current.shape[3]-target.shape[3]) - left\n",
    "    top = (current.shape[2]-target.shape[2])//2\n",
    "    down = (current.shape[2]-target.shape[2]) - top\n",
    "    croped = current[:,:,top:-down , left:-right]\n",
    "    return croped\n",
    "def padd_dims(target , current):\n",
    "    pad_h = target.shape[2] - current.shape[2] \n",
    "    pad_w = target.shape[3] - current.shape[3]\n",
    "    padded = F.pad(current, (0, pad_w, 0, pad_h), mode='constant', value=0)\n",
    "    return padded\n",
    "\n",
    "@torch.no_grad()\n",
    "def TP_TN_FP_FN(preds,gt,process_preds=True,return_TN=False):\n",
    "    if(process_preds):\n",
    "        preds_argmax = torch.argmax(preds,dim=1)\n",
    "        onehot_preds = F.one_hot(preds_argmax,num_classes=preds.shape[1])\n",
    "        pred_onehot = onehot_preds.permute(0, 3, 1, 2).float()\n",
    "    else :\n",
    "        pred_onehot = preds\n",
    "        \n",
    "    onehot_gt = F.one_hot(gt,num_classes=preds.shape[1])\n",
    "    onehot_gt = onehot_gt.permute(0, 3, 1, 2).float()\n",
    "    TN = 0\n",
    "    if(return_TN):\n",
    "        TN = (((1-onehot_gt)*(1-pred_onehot)).sum(dim=(0,2,3))).cpu()\n",
    "    TP = ((onehot_gt*pred_onehot).sum(dim=(0,2,3))).cpu()\n",
    "    \n",
    "    FP = (((1-onehot_gt)*pred_onehot).sum(dim=(0,2,3))).cpu()\n",
    "    FN = ((onehot_gt*(1-pred_onehot)).sum(dim=(0,2,3))).cpu()\n",
    "    return TP , TN , FP , FN\n",
    "\n",
    "def draw_mask(image,mask,args=None,colors=None):\n",
    "    img = image.copy()\n",
    "    if(args is not None):\n",
    "        class_count= args[\"class_count\"]\n",
    "    H,W,C=img.shape\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            c = mask[i,j]\n",
    "            if(c==0):\n",
    "                continue\n",
    "            if(colors is not None):\n",
    "                img[i,j] = colors[c-1]\n",
    "            else :\n",
    "                img[i,j] = (0,255,0)\n",
    "    # plt.imshow(image)\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def plot_some_images(data,transforms,image_counts=36,fig_shape=(6,6),base_transforms=None):\n",
    "    ds = UnetExampleDataset(transform=transforms , data=data,base_transform=base_transforms)\n",
    "    dataloader = DataLoader(\n",
    "        ds,\n",
    "        batch_size = 2 ,\n",
    "        num_workers = 4 ,\n",
    "        pin_memory=False,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    iter_loader = iter(dataloader)\n",
    "    w,h=fig_shape\n",
    "    plt.figure(figsize=(w*5,h*5))\n",
    "    for i in range(1,image_counts+1,2):\n",
    "        new_imgs , new_mask , old_imgs , old_mask = next(iter_loader)\n",
    "        new_img = new_imgs[0].numpy()\n",
    "        old_img = old_imgs[0].numpy()\n",
    "        if(new_img.shape[0]==1):\n",
    "            new_img = new_img[0]\n",
    "            old_img = old_img[0]\n",
    "\n",
    "        x_disp = (new_img- new_img.min()) / (new_img.max() - new_img.min() + 1e-8)\n",
    "        new_img = np.repeat(x_disp[..., None], 3, axis=2)*255\n",
    "        new_img = draw_mask(new_img,new_mask[0])\n",
    "\n",
    "        plt.subplot(w,h,i)\n",
    "        plt.imshow(old_img,cmap=\"gray\")\n",
    "        plt.title(\"Old Image\")\n",
    "\n",
    "        plt.subplot(w,h,i+1)\n",
    "        plt.imshow(new_img)\n",
    "        plt.title(\"New Image\")\n",
    "\n",
    "def pre_hard_skeletonize(base_path,output_path):\n",
    "    parts = [\"train\",\"val\",\"test\"]\n",
    "    os.makedirs(os.path.join(output_path , \"skels\"),exist_ok=True)\n",
    "    for part in parts:\n",
    "        mask_base_path = os.path.join(base_path,\"labels\",part)\n",
    "        os.makedirs(os.path.join(output_path , \"skels\",part),exist_ok=True)\n",
    "\n",
    "        mask_list = os.listdir(mask_base_path)\n",
    "        for mask_name in tqdm(mask_list):\n",
    "            name = Path(mask_name).stem\n",
    "            mask_path = os.path.join(mask_base_path,mask_name)\n",
    "\n",
    "            mask = zarr.load(str(mask_path))\n",
    "       \n",
    "            mask = (mask!=0).astype(np.uint8)\n",
    "        \n",
    "            out_skel_path = os.path.join(output_path,\"skels\",part,f\"{name}.png\")\n",
    "            skel = skeletonize(mask).astype(np.uint8) * 255\n",
    "            cv2.imwrite(out_skel_path,skel)\n",
    "@torch.no_grad()\n",
    "def pre_soft_skeletonize(base_path,output_path,batch_size=10,k=25):\n",
    "    parts = [\"train\",\"val\",\"test\"]\n",
    "    os.makedirs(os.path.join(output_path , \"skels_soft\"),exist_ok=True)\n",
    "    for part in parts:\n",
    "        mask_base_path = os.path.join(base_path,\"labels\",part)\n",
    "        os.makedirs(os.path.join(output_path , \"skels_soft\",part),exist_ok=True)\n",
    "\n",
    "        mask_list = os.listdir(mask_base_path)\n",
    "        mask_buffer = []\n",
    "        name_buffer = []\n",
    "        for i,mask_name in enumerate(tqdm(mask_list)):\n",
    "            name = Path(mask_name).stem\n",
    "            mask_path = os.path.join(mask_base_path,mask_name)\n",
    "            mask = zarr.load(str(mask_path))\n",
    "            mask = (mask!=0).astype(np.float32)\n",
    "\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)\n",
    "            mask_buffer.append(mask)\n",
    "            name_buffer.append(name)\n",
    "            if((i+1)%batch_size==0 or i==len(mask_list)-1):\n",
    "                mask_buffer = torch.cat(mask_buffer,dim=0).to(\"cuda\")\n",
    "                skels = soft_skeletonize(mask_buffer,k=k)\n",
    "                skels = skels.cpu().numpy()\n",
    "                B = skels.shape[0]\n",
    "                for i in range(B):\n",
    "                    skel = skels[i,0].astype(np.uint8)*255\n",
    "                    o_name = name_buffer[i]\n",
    "                    out_skel_path = os.path.join(\n",
    "                        output_path,\"skels_soft\",part,f\"{o_name}.png\")\n",
    "                    cv2.imwrite(out_skel_path,skel)\n",
    "                mask_buffer = []\n",
    "                name_buffer = []\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_confution_matrix(data_loader,model,class_maps,output_folder_path=None,draw_plot = True,class_count=26):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    conf_mat = torch.zeros((class_count,class_count))\n",
    "    model.eval()\n",
    "    for img , masks in tqdm(data_loader):\n",
    "        img = img.to(device)\n",
    "        with torch.autocast(device_type=device,dtype=torch.float16):\n",
    "            mask = masks[0].to(device).view(-1)\n",
    "            pred_masks = model(img)\n",
    "\n",
    "        pred_mask = torch.argmax(pred_masks[0],dim=1).view(-1)\n",
    "        encoded_results = (mask*class_count + pred_mask).cpu()\n",
    "        counts = torch.bincount(encoded_results,minlength=class_count**2).view(class_count,class_count)\n",
    "        conf_mat += counts\n",
    "        \n",
    "    conf_mat = conf_mat.float() / conf_mat.sum(dim=1,keepdims=True).clamp(min=1)\n",
    "    conf_mat = conf_mat.numpy()\n",
    "    if(draw_plot):\n",
    "        class_names = [\"background\" for i in range(class_count)]\n",
    "        for index , name in class_maps.items():\n",
    "            class_names[index] = name\n",
    "        plt.figure(figsize=(20,20))\n",
    "        ax = sns.heatmap(\n",
    "            conf_mat,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cmap=\"Blues\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Predicted class\")\n",
    "        ax.set_ylabel(\"True class\")\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        if(output_folder_path):\n",
    "            out_path = os.path.join(output_folder_path,\"conf_mat.png\")\n",
    "            plt.savefig(out_path)\n",
    "    return conf_mat\n",
    "\n",
    "def erode(mask):\n",
    "    h_pool = -F.max_pool2d(-mask,(3,1),(1,1),(1,0))\n",
    "    v_pool = -F.max_pool2d(-mask,(1,3),(1,1),(0,1))\n",
    "    return torch.min(v_pool,h_pool)\n",
    "def dilate(mask):\n",
    "    return F.max_pool2d(mask,(3,3),(1,1),(1,1))\n",
    "def soft_open(mask):\n",
    "    return dilate(erode(mask))\n",
    "def soft_skeletonize(I,k=25):\n",
    "    I_ = soft_open(I)\n",
    "    S = F.relu(I-I_)\n",
    "    for i in range(k):\n",
    "        I = erode(I)\n",
    "        I_ = soft_open(I)\n",
    "        S = S + (1-S)*F.relu(I-I_)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119d172",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([\n",
    "    (242,  24,  24),   # Red\n",
    "    (242,  77,  24),   # Red-Orange\n",
    "    (242, 129,  24),   # Orange\n",
    "    (242, 181,  24),   # Yellow-Orange\n",
    "    ( 24, 242, 216),   # Cyan\n",
    "    (242, 234,  24),   # Yellow\n",
    "    (146,  24, 242),   # Purple\n",
    "    (199, 242,  24),   # Yellow-Green\n",
    "    (146, 242,  24),   # Lime\n",
    "    ( 94, 242,  24),   # Green\n",
    "    (242,  24, 181),   # Fuchsia\n",
    "    ( 42, 242,  24),   # Green (brighter)\n",
    "    ( 94,  24, 242),   # Violet\n",
    "    ( 24, 242,  59),   # Spring Green\n",
    "    (242,  24, 129),   # Pink\n",
    "    ( 24, 242, 111),   # Aquamarine\n",
    "    ( 24, 242, 164),   # Turquoise\n",
    "    ( 24, 164, 242),   # Azure\n",
    "    (199,  24, 242),   # Magenta\n",
    "    ( 24, 216, 242),   # Sky Blue\n",
    "    ( 24, 111, 242),   # Blue\n",
    "    (242,  24, 234),   # Hot Pink\n",
    "    ( 24,  59, 242),   # Royal Blue\n",
    "    ( 42,  24, 242),   # Indigo\n",
    "    (242,  24,  77),   # Rose\n",
    "], dtype=np.uint8)\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_full_report(recorder,output_base_path,model,valid_loader,\n",
    "                     args,class_map,name=None):\n",
    "    now = datetime.datetime.now()\n",
    "    save_folder_name = str(now)\n",
    "    if(name):\n",
    "        save_folder_name += f\" [{name}]\"\n",
    "    output_folder_path = os.path.join(output_base_path,save_folder_name)\n",
    "\n",
    "    os.makedirs(output_folder_path,exist_ok=True)\n",
    "\n",
    "    print(\"Saving Memory\")\n",
    "    save_memory(recorder,args,output_folder_path)\n",
    "\n",
    "    print(\"Saving All Plots\")\n",
    "    draw_loss_plots(recorder , output_folder_path)\n",
    "    draw_avg_metric_plots(recorder , output_folder_path)\n",
    "    draw_all_metric_plots(recorder , output_folder_path)\n",
    "    compute_confution_matrix(\n",
    "        data_loader=valid_loader,\n",
    "        model = model,\n",
    "        class_maps = class_map,\n",
    "        draw_plot = True,\n",
    "        class_count=len(class_map)+1,\n",
    "        output_folder_path=output_folder_path\n",
    "    )\n",
    "    print(\"Saving Examples\")\n",
    "    draw_examples(model,valid_loader,args,class_map,output_folder_path)\n",
    "\n",
    "    print(\"Saving Verbal Results\")\n",
    "    write_verbal_results(recorder,output_folder_path)\n",
    "\n",
    "    print(\"Copying Notebook To Results\")\n",
    "    notebook_name = \"nnUnetAttention.ipynb\"\n",
    "\n",
    "    notebook_out_path = os.path.join(output_folder_path,\"notebook.ipynb\") \n",
    "    shutil.copyfile(f\"./{notebook_name}\",notebook_out_path )\n",
    "    print(\"builfding kaggle project\")\n",
    "    build_kaggle_project(output_folder_path)\n",
    "\n",
    "def write_verbal_results(recorder,output_base_path):\n",
    "    report = \"\"\n",
    "    report_path = os.path.join(output_base_path,\"report.txt\")\n",
    "    losses_keys = recorder.losses_keys\n",
    "    with open(\"./data/train_count.json\",\"r\") as f:\n",
    "        train_count = json.load(f)\n",
    "    for part,data in recorder.metric_avg_list.items():\n",
    "        report +=f\"======= > {part} verbal Report < =======\\n\"\n",
    "\n",
    "        dice_list = data[\"dice\"]\n",
    "        precison_list = data[\"precision\"]\n",
    "        recall_list = data[\"recall\"]\n",
    "\n",
    "        best_idx = int(np.argmax(dice_list))\n",
    "        \n",
    "        best_dice = dice_list[best_idx]\n",
    "        best_precision = precison_list[best_idx]\n",
    "        best_recall = recall_list[best_idx]\n",
    "\n",
    "\n",
    "        report += (\n",
    "            f\"best epoch : [{best_idx+1}]\\n\"\n",
    "            f\"best dice : [{best_dice}] - best precision : [{best_precision}] - best recall : [{best_recall}] \\n\"\n",
    "        )\n",
    "    \n",
    "        for loss_name in losses_keys:\n",
    "            loss_list = recorder.history[part][loss_name]\n",
    "            best_loss = loss_list[best_idx]\n",
    "\n",
    "            report += f\"bset {loss_name} : [{best_loss}] - \"\n",
    "            \n",
    "\n",
    "        for index , c in recorder.class_maps.items():\n",
    "            dice = recorder.metric_history[part][\"dice\"][index][best_idx]\n",
    "            precision = recorder.metric_history[part][\"precision\"][index][best_idx]\n",
    "            recall = recorder.metric_history[part][\"recall\"][index][best_idx]\n",
    "\n",
    "            counts = train_count[c]\n",
    "            report += f\"{c} => dice : {dice} - p : {precision} - r : {recall} || train counts : {counts}\\n\"\n",
    "        report +=\"<=><=><=><=><=><=><=><=><=><=><=><=><=><=><=><=><=>\\n\"\n",
    "    with open(report_path , \"w\") as f : \n",
    "        f.write(report)\n",
    "\n",
    "def save_memory(recorder,args,output_folder_path):\n",
    "    history_path = os.path.join(output_folder_path,\"loss_history.json\")\n",
    "    full_metric_path = os.path.join(output_folder_path,\"full_metric_hostory.json\")\n",
    "    avg_metric_path = os.path.join(output_folder_path,\"avg_metric_hostory.json\")\n",
    "    args_path = os.path.join(output_folder_path,\"args.json\")\n",
    "\n",
    "    with open(history_path , \"w\") as f:\n",
    "        json.dump(recorder.history,f,indent=4)\n",
    "    with open(full_metric_path , \"w\") as f:\n",
    "        json.dump(recorder.metric_history,f,indent=4)\n",
    "    with open(avg_metric_path , \"w\") as f:\n",
    "        json.dump(recorder.metric_avg_list,f,indent=4)\n",
    "    with open(args_path , \"w\") as f:\n",
    "        json.dump(args,f,indent=4)\n",
    "\n",
    "def draw_loss_plots(recorder,output_folder_path):\n",
    "    plt.figure(figsize=(15,20))\n",
    "    \n",
    "    losses_keys = recorder.losses_keys\n",
    "    colors = [\"g\",\"r\",\"b\",\"y\",\"orange\"]\n",
    "    colors_per_class = {}\n",
    "\n",
    "    for i,loss_name in enumerate(losses_keys):\n",
    "        colors_per_class[loss_name] = colors[i]\n",
    "\n",
    "    plt_path =os.path.join(output_folder_path,\"loss_plot.png\")\n",
    "    \n",
    "    for i,part in enumerate(recorder.history):\n",
    "        plt.subplot(2,1,i+1)\n",
    "        for loss_name,data in recorder.history[part].items():\n",
    "            length = len(data)-1\n",
    "            x = np.arange(length)\n",
    "            plt.plot(x,data[:-1],color = colors_per_class[loss_name],label=loss_name)\n",
    "        plt.title(f\"{part} loss plot\")\n",
    "        plt.legend()\n",
    "    plt.savefig(plt_path,dpi=150)\n",
    "\n",
    "def draw_avg_metric_plots(recorder,output_folder_path):\n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt_path =os.path.join(output_folder_path,\"avg_metrics.png\")\n",
    "    for i,part in enumerate(recorder.metric_avg_list):\n",
    "        plt.subplot(2,1,i+1)\n",
    "\n",
    "        dice_data = recorder.metric_avg_list[part][\"dice\"]\n",
    "        precision_data = recorder.metric_avg_list[part][\"precision\"]\n",
    "        recall_data = recorder.metric_avg_list[part][\"recall\"]\n",
    "\n",
    "        length = len(dice_data)\n",
    "        x = np.arange(length)\n",
    "        plt.plot(x,dice_data,color=\"g\",label=\"dice\")\n",
    "        plt.plot(x,precision_data,color=\"r\",label=\"precision\")\n",
    "        plt.plot(x,recall_data,color=\"b\",label=\"recall\")\n",
    "        plt.title(f\"{part} avg dice plot\")\n",
    "        plt.legend()\n",
    "    plt.savefig(plt_path)\n",
    "def draw_all_metric_plots(recorder,output_folder_path):\n",
    "    for part in recorder.history: \n",
    "        plt_path =os.path.join(output_folder_path,f\"{part}_full_metric.png\")\n",
    "        plt.figure(figsize=(30,30))\n",
    "        for i , class_index  in enumerate(recorder.metric_history[part][\"dice\"]):\n",
    "            dice_data = recorder.metric_history[part][\"dice\"][class_index]\n",
    "            precision_data = recorder.metric_history[part][\"precision\"][class_index]\n",
    "            recall_data = recorder.metric_history[part][\"recall\"][class_index]\n",
    "\n",
    "            class_name = recorder.class_maps[class_index]\n",
    "            plt.subplot(5,5,i+1)\n",
    "            length = len(dice_data)\n",
    "            x = np.arange(length)\n",
    "            plt.plot(x,dice_data,color=\"g\",label=\"dice\")\n",
    "            plt.plot(x,precision_data,color=\"r\",label=\"precision\")\n",
    "            plt.plot(x,recall_data,color=\"b\",label=\"recall\")\n",
    "            plt.title(f\"{class_name}\")\n",
    "            plt.legend()\n",
    "\n",
    "        plt.savefig(plt_path)\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def draw_examples(model,valid_loader,args,class_map,output_folder_path,w=6,h=6):\n",
    "    plt_path = os.path.join(output_folder_path,\"examples.png\")\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plot_count =18\n",
    "    patches = [\n",
    "        mpatches.Patch(color=np.array(colors[j-1]) / 255.0, label=class_map[j])\n",
    "        for j in range(1,len(class_map)+1)\n",
    "    ]\n",
    "    i=0\n",
    "    img_index=1\n",
    "    valid_iterator = iter(valid_loader)\n",
    "    model.eval()\n",
    "    for i in tqdm(range(plot_count)):\n",
    "        img , mask = next(valid_iterator)\n",
    "        with torch.autocast(device_type=args[\"device\"],dtype=torch.float16):\n",
    "            pred_masks = model(img.to(args[\"device\"]))\n",
    "        pred_mask = pred_masks[0].cpu().numpy()\n",
    "        pred_mask = np.argmax(pred_mask,axis=1)\n",
    "        mask = mask[0].numpy()\n",
    "        img = img.numpy()\n",
    "        x_disp = (img[0,0] - img[0,0].min()) / (img[0,0].max() - img[0,0].min() + 1e-8)\n",
    "        rgb = np.repeat(x_disp[..., None], 3, axis=2)*255\n",
    "        real_annoted = draw_mask(rgb,mask[0],args,colors)\n",
    "        pred_annoted = draw_mask(rgb,pred_mask[0],args,colors)\n",
    "        \n",
    "        plt.subplot(h,w,img_index)\n",
    "        plt.imshow(real_annoted)\n",
    "        plt.title(f\"Ground Truth \")\n",
    "        plt.subplot(h,w,img_index+1)\n",
    "        plt.imshow(pred_annoted)\n",
    "        plt.title(f\"Predicted \")\n",
    "        img_index+=2\n",
    "        if(img_index-1==h):\n",
    "            plt.legend(\n",
    "                handles=patches,\n",
    "                bbox_to_anchor=(1.05, 1),\n",
    "                loc='upper left',\n",
    "                borderaxespad=0.,\n",
    "                title=\"Classes\"\n",
    "            )\n",
    "        i+=1\n",
    "    plt.savefig(plt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd310ed8",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d68d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHE : \n",
    "    def __init__(self,clipLimit=2.0,tileGridSize=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    def __call__(self,img):\n",
    "        enhanced = np.clip(img, 0, 255)\n",
    "        return self.clahe.apply(enhanced)\n",
    "class WhiteTopHat:\n",
    "    def __init__(self,kernel_size = (50, 50),turn_neg = True):\n",
    "        self.kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "        self.turn_neg = turn_neg\n",
    "    def __call__(self,img):\n",
    "        neg_img = cv2.bitwise_not(img)\n",
    "        tophat_img = cv2.morphologyEx(neg_img, cv2.MORPH_TOPHAT, self.kernel,borderType=cv2.BORDER_REPLICATE)\n",
    "        # tophat_img = morphology.white_tophat(neg_img, self.kernel) \n",
    "        return cv2.subtract(img, tophat_img)\n",
    "        \n",
    "# Augementations \n",
    "def normalize_xca(img, **kwargs):\n",
    "    x = img.astype(np.float32, copy=False)\n",
    "    m = x > 0\n",
    "    if np.any(m):\n",
    "        mean = x[m].mean()\n",
    "        std  = x[m].std()\n",
    "        x[m] = (x[m] - mean) / (std + 1e-8)\n",
    "        x[~m] = 0.0\n",
    "    else:\n",
    "        x = x / 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ae845",
   "metadata": {},
   "source": [
    "# recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryRecorder:\n",
    "    def __init__(self,class_maps,losses_keys,class_count = 25):\n",
    "\n",
    "        self.history = {\n",
    "            \"train\":{},\n",
    "            \"valid\":{}\n",
    "        }\n",
    "        for key in losses_keys:\n",
    "            self.history[\"train\"][key] = [[]]\n",
    "            self.history[\"valid\"][key] = [[]]\n",
    "        self.losses_keys = losses_keys\n",
    "        self.metric_history={}\n",
    "        self.class_maps =class_maps\n",
    "        for part in self.history :\n",
    "            self.metric_history[part]={\"dice\":{},\"precision\":{},\"recall\":{}}\n",
    "            for i in range(1,class_count+1):\n",
    "                self.metric_history[part][\"dice\"][i]=[]\n",
    "                self.metric_history[part][\"recall\"][i]=[]\n",
    "                self.metric_history[part][\"precision\"][i]=[]\n",
    "                \n",
    "        self.metric_avg_list = {\n",
    "            \"train\":{\"dice\":[],\"precision\":[],\"recall\":[]},\n",
    "            \"valid\":{\"dice\":[],\"precision\":[],\"recall\":[]}\n",
    "        }\n",
    "\n",
    "        self.class_count = class_count \n",
    "        \n",
    "    def add_losses(self,part,loss_dict):\n",
    "        for loss_name,loss in loss_dict.items():\n",
    "            self.history[part][loss_name][-1] += [loss]\n",
    "        \n",
    "    def add_metrics(self,dice,precision,recall,part):\n",
    "        dice = dice[1:]\n",
    "        precision = precision[1:]\n",
    "        recall = recall[1:]\n",
    "        for i in range(self.class_count):\n",
    "            d = dice[i]\n",
    "            r = recall[i]\n",
    "            p = precision[i]\n",
    "            self.metric_history[part][\"dice\"][i+1].append(d)\n",
    "            self.metric_history[part][\"recall\"][i+1].append(r)\n",
    "            self.metric_history[part][\"precision\"][i+1].append(p)\n",
    "    def avg_losses(self,part):\n",
    "        for key in self.history[part]:\n",
    "            self.history[part][key][-1] = np.mean(self.history[part][key][-1])\n",
    "            self.history[part][key].append([])\n",
    "            \n",
    "    def print_loss_report(self,part,epoch,avg_first=True):\n",
    "        if(avg_first):\n",
    "            self.avg_losses(part)\n",
    "            \n",
    "        report = f\"{part} ==> epcoh ({epoch})\\n\"\n",
    "        co=0\n",
    "        for loss_name in self.losses_keys:\n",
    "            loss_list = self.history[part][loss_name]\n",
    "\n",
    "            loss = loss_list[-2]\n",
    "            report += f\"{loss_name} : {loss}\"\n",
    "            if((co+1)%3==0):\n",
    "                report += \"\\n\"\n",
    "            else:\n",
    "                report+=\" - \"\n",
    "            co+=1  \n",
    "                 \n",
    "        print(report)\n",
    "    def print_metrics_report(self,part,epoch,class_wise=False):\n",
    "        \n",
    "        report_temp = f\"{part} avg metrics for epoch {epoch} :\\n\"\n",
    "        report_class_wise_temp = \"\"\n",
    "        avg_dice = 0\n",
    "        avg_precision = 0\n",
    "        avg_recall = 0\n",
    "        for index , c in self.class_maps.items():\n",
    "            dice = self.metric_history[part][\"dice\"][index][-1]\n",
    "            precision = self.metric_history[part][\"precision\"][index][-1]\n",
    "            recall = self.metric_history[part][\"recall\"][index][-1]\n",
    "            avg_dice += dice\n",
    "            avg_precision += precision\n",
    "            avg_recall += recall\n",
    "            if(class_wise):\n",
    "                report_class_wise_temp += f\"{c} => dice : {dice} p : {precision} , r : {recall}\\n\"\n",
    "        \n",
    "        avg_dice = avg_dice/self.class_count\n",
    "        avg_precision = avg_precision/self.class_count\n",
    "        avg_recall = avg_recall/self.class_count\n",
    "\n",
    "        self.metric_avg_list[part][\"dice\"]+=[avg_dice]\n",
    "        self.metric_avg_list[part][\"precision\"]+=[avg_precision]\n",
    "        self.metric_avg_list[part][\"recall\"]+=[avg_recall]\n",
    "        \n",
    "        report_temp+=f\"avg dice : {avg_dice} - avg precision : {avg_precision} - avg recall : {avg_recall}\"\n",
    "\n",
    "        if(class_wise):\n",
    "            report_temp = report_temp + \"\\n\" +report_class_wise_temp[:-1] #removing last \\n\n",
    "        print(report_temp)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855812d7",
   "metadata": {},
   "source": [
    "# nnunet_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_dims():\n",
    "    pass\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self,in_c , out_c,p):\n",
    "        super(Conv,self).__init__()\n",
    "        self.layers =  nn.Sequential( \n",
    "            nn.Conv2d(\n",
    "                in_channels = in_c , \n",
    "                out_channels = out_c ,\n",
    "                kernel_size=3, \n",
    "                stride = 1 ,\n",
    "                padding = p\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_c, eps=1e-5, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=1e-2, inplace=True)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "class DownsampleConv(nn.Module):\n",
    "    def __init__(self,in_c , out_c):\n",
    "        super(DownsampleConv,self).__init__()\n",
    "        self.layers =  nn.Sequential( \n",
    "            nn.Conv2d(\n",
    "                in_channels = in_c , \n",
    "                out_channels = out_c ,\n",
    "                kernel_size=3, \n",
    "                stride = 2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_c, eps=1e-5, affine=True),\n",
    "            nn.LeakyReLU(negative_slope=1e-2, inplace=True)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,in_c , out_c,p=1):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv(in_c = in_c , out_c = out_c , p=p),\n",
    "            Conv(in_c = out_c , out_c=out_c ,p=p)\n",
    "        )\n",
    "        self.pool =  DownsampleConv(in_c = out_c , out_c=out_c )\n",
    "    def forward(self,x):\n",
    "        z = self.layers(x)\n",
    "        return z , self.pool(z)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,in_c ,out_c , f_int_scale,class_count,gate_c = None , attention=False,dsv=False):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.dsv=dsv\n",
    "        self.conv1 = Conv(in_c=in_c , out_c=out_c,p=1)\n",
    "        self.conv2 = Conv(in_c = out_c , out_c = out_c , p=1)\n",
    "        self.upsampler = nn.ConvTranspose2d(\n",
    "            in_channels = out_c , \n",
    "            out_channels = out_c//2 ,\n",
    "            kernel_size=2 ,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        if(self.dsv):\n",
    "            self.dsv_block = nn.Conv2d(in_channels=out_c,out_channels=class_count,kernel_size=1)\n",
    "        #in_c * 2 = gate_c\n",
    "        if(attention):\n",
    "            self.gate = AttentionGate(gate_in_c=gate_c ,f_int_scale=f_int_scale,skip_in_c=in_c//2)\n",
    "        self.attention=attention\n",
    "    def forward(self,x_in,x_skip,x_gate):\n",
    "        if(self.attention):\n",
    "            x_skip = self.gate(x_skip , x_gate)\n",
    "        if(x_in.shape[2] != x_skip.shape[2] or x_in.shape[3] != x_skip.shape[3]):\n",
    "            x_skip = crop_dims(x_in,x_skip)\n",
    "        \n",
    "        x = torch.cat([x_skip,x_in],dim=1)\n",
    "        z = self.conv1(x)\n",
    "        gate_z = self.conv2(z)\n",
    "        upsampled_z = self.upsampler(gate_z)\n",
    "        if(self.dsv):\n",
    "            dsv_out = self.dsv_block(gate_z)\n",
    "            \n",
    "            if(self.attention):\n",
    "               \n",
    "                return upsampled_z , gate_z , dsv_out\n",
    "            else:\n",
    "                return upsampled_z , None, dsv_out \n",
    "        else:\n",
    "            if(self.attention):\n",
    "                return upsampled_z , gate_z , None\n",
    "            else:\n",
    "                return upsampled_z , None , None\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self,in_c , out_c,p,attention=False):\n",
    "        super(BottleNeck,self).__init__()\n",
    "        self.conv1 = Conv(in_c=in_c , out_c=out_c,p=p)\n",
    "        self.conv2 = Conv(in_c = out_c , out_c = out_c , p=p)\n",
    "        self.upsampler = nn.ConvTranspose2d(\n",
    "            in_channels = out_c , \n",
    "            out_channels = out_c//2 ,\n",
    "            kernel_size=2 ,\n",
    "            stride=2\n",
    "        )\n",
    "        self.attention=attention\n",
    "    def forward(self,x):\n",
    "        z = self.conv1(x)\n",
    "        gate_z = self.conv2(z)\n",
    "        upsampled_z = self.upsampler(gate_z)\n",
    "        if(self.attention):\n",
    "            return upsampled_z , gate_z\n",
    "        return upsampled_z , None\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self,gate_in_c,skip_in_c,f_int_scale=2,f_int=None,scaler=\"sigmoid\"):\n",
    "        super(AttentionGate,self).__init__()\n",
    "        f_int = min(gate_in_c//f_int_scale,skip_in_c//f_int_scale) if f_int==None else f_int\n",
    "        f_int = 1 if f_int == 0 else f_int\n",
    "        self.conv_gate = nn.Conv2d(in_channels = gate_in_c , out_channels = f_int , \n",
    "                                   kernel_size = 1)\n",
    "        self.conv_skip = nn.Conv2d(in_channels = skip_in_c , out_channels = f_int , \n",
    "                                   kernel_size = 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_shrink = nn.Conv2d(in_channels = f_int , out_channels = 1 ,\n",
    "                                     kernel_size = 1)\n",
    "        if(scaler ==\"sigmoid\"):\n",
    "            self.scaler = nn.Sigmoid()    \n",
    "    def forward(self,x_skip,x_gate):\n",
    "        x_gate_int = self.conv_gate(x_gate)\n",
    "        x_skip_int = self.conv_skip(x_skip)\n",
    "        # x_skip_int = crop_dims(x_gate_int,x_skip_int)\n",
    "        if x_skip_int.shape[2:] != x_gate_int.shape[2:]:\n",
    "            x_skip_int = F.interpolate(\n",
    "                x_skip_int, \n",
    "                size=x_gate_int.shape[2:], \n",
    "                mode=\"bilinear\", \n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        added_x = x_skip_int + x_gate_int\n",
    "        relu_x = self.relu(added_x)\n",
    "        shrinked_x = self.conv_shrink(relu_x)\n",
    "        sig_x = self.scaler(shrinked_x)\n",
    "        # padded_x = padd_dims(x_skip , sig_x)\n",
    "        if sig_x.shape[2:] != x_skip.shape[2:]:\n",
    "            padded_x = F.interpolate(\n",
    "                sig_x, \n",
    "                size=x_skip.shape[2:], \n",
    "                mode=\"bilinear\", \n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "        return padded_x*x_skip\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,in_c ,out_c ,class_count ,f_int_scale, \n",
    "        gate_c = None , attention=False):\n",
    "\n",
    "        super(Head,self).__init__()\n",
    "        self.conv1 = Conv(in_c=in_c , out_c=out_c,p=1)\n",
    "        self.conv2 = Conv(in_c = out_c , out_c = out_c , p=1)\n",
    "        self.conv1x1 = nn.Conv2d(\n",
    "            in_channels = out_c , \n",
    "            out_channels = class_count ,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        if(attention):\n",
    "            self.gate = AttentionGate(\n",
    "                gate_in_c=gate_c , \n",
    "                f_int_scale=f_int_scale,\n",
    "                skip_in_c=in_c//2\n",
    "            )\n",
    "        self.attention=attention\n",
    "    def forward(self,x_in,x_skip,x_gate):\n",
    "        if(self.attention):\n",
    "            x_skip = self.gate(x_skip , x_gate)\n",
    "        if(x_in.shape[2] != x_skip.shape[2] or x_in.shape[3] != x_skip.shape[3]):\n",
    "            x_skip = crop_dims(x_in,x_skip)\n",
    "            \n",
    "        x = torch.cat([x_skip,x_in],dim=1)\n",
    "        z = self.conv1(x)\n",
    "        gate_z = self.conv2(z)\n",
    "        \n",
    "        class_feature_maps = self.conv1x1(gate_z) \n",
    "        \n",
    "        return class_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e8f98",
   "metadata": {},
   "source": [
    "# nnunet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnUnet(nn.Module):\n",
    "    def __init__(self,args,encoder_channel_settings=None,decoder_channel_settings=None):\n",
    "        super(nnUnet,self).__init__()\n",
    "        \n",
    "        in_c = args[\"in_c\"]\n",
    "        class_count = args[\"class_count\"]\n",
    "        attention = args[\"attention\"]\n",
    "        image_shape = args[\"image_shape\"]\n",
    "        base_channel = args[\"base_channel\"]\n",
    "        f_int_scale = args[\"f_int_scale\"]\n",
    "        max_channels = args[\"max_channels\"]\n",
    "        input_channels = args[\"input_channels\"]\n",
    "        self.deep_super_vision = args[\"deep_super_vision\"]\n",
    "        h = image_shape[0]\n",
    "        w = image_shape[1]\n",
    "        \n",
    "        max_pool_count = 0\n",
    "        co=0\n",
    "        \n",
    "        while(w>4 and h>4):\n",
    "            w/=2\n",
    "            h/=2\n",
    "            co+=1\n",
    "        print(f\"number of layers : {co}\")\n",
    "\n",
    "        # create encoder settings \n",
    "        if(encoder_channel_settings is None):\n",
    "            self.encoder_channel_settings = [base_channel]\n",
    "            for i in range(co-1):\n",
    "                new_c =min(self.encoder_channel_settings[i]*2,max_channels)\n",
    "                self.encoder_channel_settings +=[new_c]\n",
    "        else :\n",
    "            self.encoder_channel_settings = encoder_channel_settings\n",
    "        \n",
    "        # create bottleneck settings\n",
    "        self.bottle_neck_channel_setting = self.encoder_channel_settings[-1]*2\n",
    "        # create decoder settings \n",
    "        if(decoder_channel_settings is  None):\n",
    "            self.decoder_channel_settings =[]\n",
    "            for i in range(co-1):\n",
    "                self.decoder_channel_settings = [self.encoder_channel_settings[i]*2] +  self.decoder_channel_settings\n",
    "        else :\n",
    "            self.decoder_channel_settings = decoder_channel_settings\n",
    "\n",
    "        \n",
    "        # build encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "        for i in range(co):\n",
    "            output_channels = self.encoder_channel_settings[i]\n",
    "            self.encoders.append(EncoderBlock(in_c=input_channels,out_c=output_channels , p=1))\n",
    "            input_channels = output_channels\n",
    "        # build bottleneck\n",
    "\n",
    "        self.bottle_neck = BottleNeck(in_c = output_channels ,out_c = self.bottle_neck_channel_setting , p=1,attention = attention)\n",
    "        #build decoder\n",
    "        input_channels = self.bottle_neck_channel_setting\n",
    "        self.decoders = []\n",
    "        for i in range(co-1):\n",
    "            \n",
    "            output_channels = self.decoder_channel_settings[i]\n",
    "\n",
    "            self.decoders = [\n",
    "                DecoderBlock(\n",
    "                    in_c = input_channels , \n",
    "                    out_c=output_channels , \n",
    "                    gate_c = input_channels , \n",
    "                    attention = attention,\n",
    "                    f_int_scale=f_int_scale,\n",
    "                    dsv = self.deep_super_vision,\n",
    "                    class_count=class_count\n",
    "                )] + self.decoders\n",
    "            \n",
    "            input_channels = output_channels\n",
    "\n",
    "\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.head = Head(\n",
    "            in_c = input_channels , \n",
    "            out_c=input_channels//2 ,\n",
    "            class_count = class_count,\n",
    "            gate_c = input_channels , \n",
    "            attention = False,\n",
    "            f_int_scale=f_int_scale\n",
    "        )\n",
    "        print(\"encoder settings : \", self.encoder_channel_settings)\n",
    "        print(\"bottle-neck settings : \", self.bottle_neck_channel_setting)\n",
    "        print(\"decoder settings : \", self.decoder_channel_settings)\n",
    "        print(\"head settings : \",class_count)\n",
    "    def forward(self,x):\n",
    "        skips = []\n",
    "        for encoder in self.encoders : \n",
    "            skip , out = encoder(x)\n",
    "            skips += [skip]\n",
    "            x = out\n",
    "        x_in,gate_in = self.bottle_neck(x)\n",
    "        \n",
    "        outputs = []\n",
    "        # print(len(self.decoders))\n",
    "        for i in range(len(self.decoders) - 1, -1, -1):\n",
    "            # print(\"2\")\n",
    "            decoder = self.decoders[i]\n",
    "            skip = skips[i+1]\n",
    "            x_out,gate_out,dsv_out = decoder(x_in,skip,gate_in)\n",
    "            \n",
    "            if(dsv_out!=None):\n",
    "                outputs = [dsv_out] + outputs\n",
    "            x_in=x_out\n",
    "            gate_in=gate_out\n",
    "        # print(x_in.shape)\n",
    "        outputs = [self.head(\n",
    "            x_in = x_in,\n",
    "            x_skip = skips[0],\n",
    "            x_gate = gate_in\n",
    "        )] + outputs\n",
    "        return outputs\n",
    "if __name__ == \"__main__\":\n",
    "    args = {\n",
    "        \"base_path\" : \"../arcade/nnUnet_dataset/syntax\",\n",
    "        \"in_c\" : 1,\n",
    "        \"base_channel\" :32,\n",
    "        \"image_shape\" : (512,512),\n",
    "        \"class_count\" : 26 ,\n",
    "        \"attention\" : True,\n",
    "        \"k\":40,\n",
    "        \"batch_size\" : 10,\n",
    "        \"num_workers\" : 10,\n",
    "        \"device\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"lr\" : 0.01,\n",
    "        \"momentum\" : 0.99,\n",
    "        \"weight_decay\" : 3e-5,\n",
    "        \"epcohs\":30,\n",
    "        \"f_int_scale\" : 2,\n",
    "        \"full_report_cycle\" : 10,\n",
    "        \"max_channels\":512,\n",
    "        \"input_channels\":1,\n",
    "        \"loss_type\":\"dice loss\",\n",
    "        \"alpha\":0.75,\n",
    "        \"beta\":0.25,\n",
    "        \"gamma\":1.00,\n",
    "        \"f_gamma\":2.0,\n",
    "        \"f_loss_scale\":1,\n",
    "        \"loss_coefs\":{\"CE\":1.0,\"Second\":1.0},\n",
    "        \"output_base_path\" : \"./outputs\",\n",
    "        \"name\" : \"Attention7-AllClass\",\n",
    "        \"deep_super_vision\" : True\n",
    "    }\n",
    "    class_map = {\n",
    "        1: '1',2: '2', 3: '3',4: '4',\n",
    "        5: '5',6: '6',7: '7',8: '8',\n",
    "        9: '9',10: '9a',11: '10',12: '10a',\n",
    "        13: '11',14: '12',15: '12a',16: '13',\n",
    "        17: '14',18: '14a',19: '15',20: '16',\n",
    "        21: '16a',22: '16b',23: '16c',\n",
    "        24: '12b',25: '14b'\n",
    "    }\n",
    "    model = nnUnet(args).to(\"cuda\")\n",
    "    ls = torch.ones((10,1,512,512)).float().to(\"cuda\")\n",
    "    outs = model(ls)\n",
    "    # for out in outs:\n",
    "    #     print(out.shape)\n",
    "\n",
    "    # \"\"\"\n",
    "    # torch.Size([10, 32, 256, 256])\n",
    "    # torch.Size([10, 64, 128, 128])\n",
    "    # torch.Size([10, 128, 64, 64])\n",
    "    # torch.Size([10, 256, 32, 32])\n",
    "    # torch.Size([10, 512, 16, 16])\n",
    "    # torch.Size([10, 512, 8, 8])\n",
    "    # torch.Size([10, 512, 4, 4])\n",
    "    # \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f908ad4",
   "metadata": {},
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetLoss(nn.Module):\n",
    "    def __init__(self,args,eps = 1e-8):\n",
    "        super(UnetLoss,self).__init__()\n",
    "        self.class_count = args[\"class_count\"]\n",
    "        self.loss_type = args[\"loss_type\"]\n",
    "        self.alpha = args[\"alpha\"]\n",
    "        self.beta = args[\"beta\"]\n",
    "        self.t_gamma = args[\"t_gamma\"]\n",
    "        self.f_gamma = args[\"f_gamma\"]\n",
    "        self.k = args[\"k\"]\n",
    "        self.loss_coefs = args[\"loss_coefs\"]\n",
    "        # self.focal_fn = FocalCrossEntropy(\n",
    "        #     f_gamma=self.f_gamma,\n",
    "        #     eps=eps,\n",
    "        #     f_alpha=args[\"f_alpha\"],\n",
    "        #     f_loss_scale = args[\"f_loss_scale\"]\n",
    "        # )\n",
    "        if(args[\"f_alpha\"] is not None):\n",
    "            w = torch.tensor(args[\"f_alpha\"],dtype=torch.float32,device=\"cuda\")\n",
    "            self.ce_fn = nn.CrossEntropyLoss(weight=w)\n",
    "        else :\n",
    "            self.ce_fn = nn.CrossEntropyLoss()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.eps = eps\n",
    "        self.sum_dims = (0,2,3)\n",
    "        if(self.loss_type==\"dice loss\"):\n",
    "            print(\"loss is set to dice\")\n",
    "            self.loss_fn = DiceLoss(self.eps,self.sum_dims)\n",
    "        elif(self.loss_type==\"tversky loss\"):\n",
    "            print(\"loss is set to tversky\")\n",
    "            self.loss_fn = TverskyLoss(self.eps,self.sum_dims,self.alpha,self.beta,self.t_gamma)\n",
    "        self.cldice_fn = CLDiceLoss(sum_dims=self.sum_dims,eps=self.eps,k=self.k) \n",
    "    def forward(self,pred_mask , gt_mask ):\n",
    "        \n",
    "        onehot_mask = F.one_hot(gt_mask, num_classes=self.class_count)\n",
    "        onehot_mask = onehot_mask.permute(0, 3, 1, 2).float()  \n",
    "\n",
    "        prob = self.softmax(pred_mask)\n",
    "\n",
    "        # Cross Entropy Loss\n",
    "        ce_loss = self.ce_fn(pred_mask,gt_mask)\n",
    "        # Dice/Tversky Loss\n",
    "        forground_prob = prob[:,1:]\n",
    "        forground_onehot_mask = onehot_mask[:,1:]\n",
    "        # present_class = forground_onehot_mask.sum(dim=self.sum_dims)>0\n",
    "        \n",
    "        second_loss = self.loss_fn(\n",
    "            pred_probs = forground_prob,\n",
    "            gt = forground_onehot_mask\n",
    "        )\n",
    "\n",
    "        ce_loss = self.loss_coefs[\"CE\"]*ce_loss\n",
    "        second_loss = self.loss_coefs[\"Second\"]*second_loss\n",
    "\n",
    "        total_loss = second_loss + ce_loss\n",
    "\n",
    "        loss_dict = {\n",
    "            \"CE loss\" : ce_loss,\n",
    "            self.loss_type : second_loss\n",
    "        }\n",
    "        return total_loss , loss_dict\n",
    "\n",
    "class FocalCrossEntropy(nn.Module):\n",
    "    def __init__(self,f_gamma,eps,f_loss_scale=1,f_alpha=None):\n",
    "        super(FocalCrossEntropy,self).__init__()\n",
    "        self.f_gamma = f_gamma\n",
    "        if(f_alpha is not None):\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.f_alpha = torch.tensor(f_alpha).to(device)\n",
    "        else:\n",
    "            self.f_alpha = f_alpha\n",
    "        self.eps = eps\n",
    "        self.f_loss_scale = f_loss_scale\n",
    "    def forward(self,prob,onehot_mask):\n",
    "        # prob : (B,C,H,W)\n",
    "        # onehot_mask : (B,C,H,W)\n",
    "        # gt_mask = (B,H,W)\n",
    "\n",
    "        p = (prob*onehot_mask).sum(dim=1) # (B,H,W)\n",
    "        pt = torch.clamp(p,self.eps,1-self.eps)\n",
    "        focal_weights = (1-pt)**self.f_gamma\n",
    "        focal_loss = focal_weights*(torch.log(pt))\n",
    "        if(self.f_alpha is not None):\n",
    "            alpha_b = self.f_alpha.view(1, -1, 1, 1).type_as(prob)\n",
    "            class_w = (alpha_b*onehot_mask).sum(dim=1)\n",
    "        else :\n",
    "            class_w = 1.0\n",
    "        return -self.f_loss_scale*(class_w*focal_loss).mean()\n",
    "\n",
    "class CLDiceLoss(nn.Module):\n",
    "    def __init__(self,eps,sum_dims,k=40):\n",
    "        super(CLDiceLoss,self).__init__()\n",
    "        self.k=k\n",
    "        self.eps = eps\n",
    "        self.sum_dims = (1,2)\n",
    "\n",
    "    def forward(self,pred_binary_mask , gt_mask,gt_skel):\n",
    "\n",
    "        binary_pred = (pred_binary_mask>=0.5).type_as(pred_binary_mask)\n",
    "        binary_gt = (gt_mask!=0).type_as(gt_mask)\n",
    "\n",
    "        pred_skel = soft_skeletonize(binary_pred,k=self.k)\n",
    "\n",
    "        t_prec = (pred_skel*binary_gt + self.eps).sum(dim=self.sum_dims)/(pred_skel.sum(dim=self.sum_dims) +self.eps)\n",
    "        t_rec = (gt_skel*binary_pred + self.eps).sum(dim=self.sum_dims)/(gt_skel.sum(dim=self.sum_dims) +self.eps)\n",
    "        \n",
    "        cldice = 2*((t_prec*t_rec)/(t_prec+t_rec))\n",
    "        cldice_loss = 1 - cldice.mean()\n",
    "        return cldice_loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self,eps,sum_dims):\n",
    "        super(DiceLoss,self).__init__()\n",
    "        self.eps = eps\n",
    "        self.sum_dims = sum_dims\n",
    "    def forward(self,pred_probs,gt):\n",
    "        tp = (gt * pred_probs).sum(dim=self.sum_dims)\n",
    "        fp = ((1-gt)*pred_probs).sum(dim=self.sum_dims)\n",
    "        fn = ((1-pred_probs)*gt).sum(dim=self.sum_dims)\n",
    "        per_class_dice_score = (2*tp +self.eps)/(2*tp + fp + fn + self.eps)\n",
    "        # if(present_class is None):\n",
    "        #     dice_loss = -per_class_dice_score.mean()\n",
    "        # else:\n",
    "        #     dice_loss = -per_class_dice_score[present_class].mean()\n",
    "        dice_loss = -per_class_dice_score.mean()\n",
    "        return dice_loss\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self,eps,sum_dims,alpha=0.3,beta=0.7,gamma=1.33):\n",
    "        super(TverskyLoss,self).__init__()\n",
    "        self.eps = eps\n",
    "        self.sum_dims = sum_dims\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "    def forward(self,pred_probs,gt):\n",
    "        tp = (gt * pred_probs).sum(dim=self.sum_dims)\n",
    "        fp = ((1-gt)*pred_probs).sum(dim=self.sum_dims)\n",
    "        fn = ((1-pred_probs)*gt).sum(dim=self.sum_dims)\n",
    "        t_index = (tp + self.eps) / (tp + self.alpha*fp + self.beta*fn + self.eps) \n",
    "\n",
    "        t_index = t_index.mean()\n",
    "        \n",
    "        return (1 - t_index)**self.gamma \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f53a7",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model,img,gt_masks,optimizer,loss_fn,scaler,args,device,loss_weights=[1]):\n",
    "    optimizer.zero_grad()\n",
    "    loss_dict={}\n",
    "    with torch.autocast(device_type=args[\"device\"],dtype=torch.float16):\n",
    "        pred_masks =  model(img)\n",
    "        loss = 0\n",
    "        for i,pred_mask in enumerate(pred_masks) : \n",
    "\n",
    "            gt_mask = gt_masks[i].to(device)\n",
    "            loss_weight = loss_weights[i]\n",
    "            layer_loss , layer_loss_dict = loss_fn(pred_mask , gt_mask)\n",
    "            if(i==0):\n",
    "                loss_dict = layer_loss_dict\n",
    "                pred_mask_last = pred_mask.detach()\n",
    "                gt_mask_last = gt_mask\n",
    "            else:\n",
    "                loss_dict = {key : loss_dict[key]+ loss_weight*layer_loss_dict[key] for key in layer_loss_dict}\n",
    "\n",
    "            loss += loss_weight*layer_loss\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "    if random.random() < 0.01:\n",
    "        with torch.no_grad():\n",
    "            print(\"--- Total Norm ---\")\n",
    "            print(total_norm)\n",
    "            # print(\"\\n--- Gradient norms ---\")\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         grad_norm = param.grad.data.norm().item()\n",
    "            #         print(f\"{name:30s}: {grad_norm:.6f}\")\n",
    "            # print(\"----------------------\\n\")\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    loss = loss.detach().cpu().item()\n",
    "    for loss_name in loss_dict:\n",
    "        loss_dict[loss_name] = loss_dict[loss_name].detach().cpu().item()\n",
    "    \n",
    "    loss_dict[\"total loss\"] = loss\n",
    "    return loss_dict , pred_mask_last , gt_mask_last\n",
    "    \n",
    "def trainer(args,recorder,model,optimizer,loss_fn,train_loader,valid_loader,loss_weights=[1]):\n",
    "    device = args[\"device\"]\n",
    "    epcohs = args[\"epcohs\"]\n",
    "    class_count = args[\"class_count\"]\n",
    "    full_report_cycle = args[\"full_report_cycle\"]\n",
    "    scaler = torch.amp.GradScaler(device = device) \n",
    "    for ep in tqdm(range(epcohs)):\n",
    "        total_TP =  torch.zeros(class_count)\n",
    "        total_FP = torch.zeros(class_count)\n",
    "        total_FN = torch.zeros(class_count)\n",
    "        \n",
    "        model.train()\n",
    "        class_wise_report = False\n",
    "        for img , gt_masks  in tqdm(train_loader) : \n",
    "            # gt_mask = gt_mask.long()\n",
    "            img = img.to(device)\n",
    "            # gt_mask = gt_mask.to(device)\n",
    "\n",
    "            loss_dict , pred_mask , gt_mask = train_fn(\n",
    "                model = model,\n",
    "                img = img,\n",
    "                gt_masks = gt_masks,\n",
    "                optimizer = optimizer,\n",
    "                loss_fn = loss_fn,\n",
    "                scaler = scaler,\n",
    "                args = args,\n",
    "                device = device,\n",
    "                loss_weights = loss_weights\n",
    "            )\n",
    "            \n",
    "            TP , _ , FP , FN = TP_TN_FP_FN(pred_mask,gt_mask,process_preds=True)\n",
    "            total_TP += TP\n",
    "            total_FP += FP\n",
    "            total_FN += FN\n",
    "            \n",
    "            recorder.add_losses(\"train\",loss_dict)\n",
    "            \n",
    "        dice_score = (2 * total_TP + 1e-8) / (2 * total_TP + total_FP + total_FN + 1e-8)\n",
    "        precision = total_TP /(total_FP + total_TP + 1e-8) \n",
    "        recall = total_TP /(total_FN + total_TP + 1e-8) \n",
    "        \n",
    "        recorder.add_metrics(\n",
    "            dice_score.tolist(),\n",
    "            precision.tolist(),\n",
    "            recall.tolist(),\n",
    "            part = \"train\"\n",
    "        )\n",
    "        \n",
    "        recorder.print_loss_report(\"train\",ep)\n",
    "        recorder.print_metrics_report(\"train\",ep,class_wise=False)\n",
    "        print(\"<=>\"*20)\n",
    "        \n",
    "        if((ep+1)%full_report_cycle==0):\n",
    "            class_wise_report=True\n",
    "            \n",
    "        evaluation(\n",
    "            recorder=recorder,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            valid_loader=valid_loader,\n",
    "            class_wise_report=class_wise_report,\n",
    "            class_count = class_count,\n",
    "            epoch=ep,\n",
    "            device=device)\n",
    "            \n",
    "@torch.no_grad()\n",
    "def evaluation(recorder,model,loss_fn,valid_loader,class_count,class_wise_report=False,epoch=None,device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total_TP = total_TP = torch.zeros(class_count)\n",
    "    total_FP = torch.zeros(class_count)\n",
    "    total_FN = torch.zeros(class_count)\n",
    "    \n",
    "    for img , gt_mask  in valid_loader:\n",
    "        img = img.to(device)\n",
    "        gt_mask = gt_mask[0].to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device,dtype=torch.float16):\n",
    "            pred_mask  = model(img)[0]\n",
    "            loss , loss_dict = loss_fn(pred_mask , gt_mask)\n",
    "\n",
    "            loss = loss.detach().cpu().item()\n",
    "            for loss_name in loss_dict:\n",
    "                loss_dict[loss_name] = loss_dict[loss_name].detach().cpu().item()\n",
    "        \n",
    "            loss_dict[\"total loss\"] = loss\n",
    "        \n",
    "        TP , _ , FP , FN = TP_TN_FP_FN(pred_mask,gt_mask,process_preds=True)\n",
    "        total_TP += TP\n",
    "        total_FP += FP\n",
    "        total_FN += FN\n",
    "        \n",
    "        recorder.add_losses(\"valid\",loss_dict)\n",
    "        \n",
    "    dice_score = (2 * total_TP + 1e-8) / (2 * total_TP + total_FP + total_FN + 1e-8)\n",
    "    precision = total_TP /(total_FP + total_TP + 1e-8) \n",
    "    recall = total_TP /(total_FN + total_TP + 1e-8) \n",
    "\n",
    "    recorder.add_metrics(\n",
    "        dice_score.tolist(),\n",
    "        precision.tolist(),\n",
    "        recall.tolist(),\n",
    "        part = \"valid\"\n",
    "    )\n",
    "    recorder.print_loss_report(\"valid\",epoch)\n",
    "    recorder.print_metrics_report(\"valid\",epoch,class_wise=class_wise_report)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f6c9f",
   "metadata": {},
   "source": [
    "# temp_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Training\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"base_path\" : \"../arcade/nnUnet_dataset/syntax\",\n",
    "    \"in_c\" : 1,\n",
    "    \"base_channel\" :32,\n",
    "    \"image_shape\" : (512,512),\n",
    "    \"class_count\" : 26 ,\n",
    "    \"attention\" : True,\n",
    "    \"k\":40,\n",
    "    \"batch_size\" : 10,\n",
    "    \"num_workers\" : 10,\n",
    "    \"device\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"lr\" : 0.001,\n",
    "    \"momentum\" : 0.99,\n",
    "    \"weight_decay\" : 3e-5,\n",
    "    \"epcohs\":100,\n",
    "    \"f_int_scale\" : 2,\n",
    "    \"full_report_cycle\" : 10,\n",
    "    \"max_channels\":512,\n",
    "    \"input_channels\":1,\n",
    "    \"loss_type\":\"tversky loss\",\n",
    "    \"alpha\":0.3,\n",
    "    \"beta\":0.7,\n",
    "    \"t_gamma\":2.00,\n",
    "    \"f_gamma\":2.0,\n",
    "    \"f_loss_scale\":1,\n",
    "    \"loss_coefs\":{\"CE\":1.0,\"Second\":1.0},\n",
    "    \"output_base_path\" : \"./outputs\",\n",
    "    \"name\" : \"Attention7-DSV-tev\",\n",
    "    \"deep_super_vision\" : True,\n",
    "    \"f_alpha\":None\n",
    "}\n",
    "class_map = {\n",
    "    1: '1',2: '2', 3: '3',4: '4',\n",
    "    5: '5',6: '6',7: '7',8: '8',\n",
    "    9: '9',10: '9a',11: '10',12: '10a',\n",
    "    13: '11',14: '12',15: '12a',16: '13',\n",
    "    17: '14',18: '14a',19: '15',20: '16',\n",
    "    21: '16a',22: '16b',23: '16c',\n",
    "    24: '12b',25: '14b'\n",
    "}\n",
    "# losses_keys = [\"total loss\",\"FCE loss\",args[\"loss_type\"]]\n",
    "losses_keys = [\"total loss\",\"CE loss\",args[\"loss_type\"]]\n",
    "out_counts = 7 if args[\"deep_super_vision\"] else 1\n",
    "loss_weights = [1/(2**i) for i in range(out_counts)]\n",
    "loss_weights\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# with open(\"./data/train_class_counts.json\",\"r\") as f:\n",
    "#     train_class_counts = json.load(f)\n",
    "\n",
    "# b = 0.999999\n",
    "\n",
    "# counts = [0]*(len(train_class_counts))\n",
    "# for k,v in train_class_counts.items():\n",
    "#     counts[int(k)] = int(v)\n",
    "# counts = np.array(counts,dtype=np.float64)\n",
    "\n",
    "# f_alpha = (1-b)/(1-np.power(b,counts))\n",
    "# f_alpha = f_alpha / f_alpha.sum()\n",
    "# f_alpha[12] = 0.25\n",
    "# # args[\"f_alpha\"] = f_alpha.tolist()\n",
    "# args[\"f_alpha\"]=None\n",
    "# args[\"f_alpha\"]\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "b=0.999\n",
    "train_class_counts = [\n",
    "    1200,374,375,369,303,525,525,\n",
    "    340,310,198,70,21,1,320,61,\n",
    "    129,305,107,49,38,232,43,48,31,63,127\n",
    "]\n",
    "# f_alpha = (1-b)/(1-np.power(b,train_class_counts))\n",
    "total = np.sum(train_class_counts)\n",
    "f_alpha = np.log(total/np.array(train_class_counts))\n",
    "f_alpha = (f_alpha / f_alpha.mean()).tolist()\n",
    "f_alpha\n",
    "args[\"f_alpha\"] = f_alpha\n",
    "f_alpha\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# pre_soft_skeletonize(args[\"base_path\"],output_path=args[\"base_path\"],batch_size=10,k=40)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.GaussianBlur(\n",
    "        sigma_limit=[0.1,0.5],\n",
    "        p=0.5\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.1,\n",
    "        contrast_limit=0.15,\n",
    "        brightness_by_max=True,\n",
    "        p=0.3\n",
    "    ),\n",
    "    A.RandomGamma(\n",
    "        gamma_limit=(90, 120), \n",
    "        p=0.3\n",
    "    ),\n",
    "    A.Rotate(limit=15, p=0.3 , fill_mask = 0),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.Lambda(image=normalize_xca),\n",
    "    ]\n",
    ")\n",
    "test_transforms = A.Compose([\n",
    "    A.Lambda(image=normalize_xca),\n",
    "    ToTensorV2()\n",
    "    ]  \n",
    ")\n",
    "# train_preprocess = v2.Compose([\n",
    "#     WhiteTopHat(kernel_size=(50,50)),\n",
    "#     CLAHE()\n",
    "\n",
    "# ])\n",
    "train_preprocess = None\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "train_images = read_images(base_path = args[\"base_path\"],preprocessor = train_preprocess,part = \"train\")\n",
    "valid_images = read_images(base_path = args[\"base_path\"],preprocessor = train_preprocess,part = \"val\")\n",
    "\n",
    "train_ds = UnetDataset(transform = train_transforms,data = train_images,base_size=args[\"image_shape\"][0],out_counts=out_counts)\n",
    "valid_ds = ValidUnetDataset(transform = test_transforms,data = valid_images)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = args[\"batch_size\"] ,\n",
    "    num_workers = args[\"num_workers\"] ,\n",
    "    pin_memory=True,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size = args[\"batch_size\"] ,\n",
    "    num_workers = args[\"num_workers\"] ,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# plot_some_images(train_images, train_transforms, image_counts=36, fig_shape=(6,6), base_transforms=test_transforms)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "model = nnUnet(args).to(args[\"device\"])\n",
    "loss_fn = UnetLoss(args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     model.parameters(),\n",
    "#     momentum=args[\"momentum\"],\n",
    "#     lr=args[\"lr\"],\n",
    "#     nesterov=True,\n",
    "#     weight_decay=args[\"weight_decay\"]\n",
    "# )\n",
    "recorder = HistoryRecorder(losses_keys=losses_keys,class_maps =class_map)\n",
    "\n",
    "trainer(\n",
    "    args=args,\n",
    "    recorder = recorder,\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    valid_loader = valid_loader,\n",
    "    loss_weights=loss_weights)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "save_full_report(\n",
    "    recorder= recorder , \n",
    "    output_base_path=args[\"output_base_path\"],\n",
    "    model=model,\n",
    "    valid_loader=valid_loader,\n",
    "    args=args,\n",
    "    class_map=class_map,\n",
    "    name=args[\"name\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
